---
title: Thread Hierarchy
---

![The thread hierarchy of the [CUDA programming model](/device-software/cuda-programming-model) spans from individual [threads](/device-software/thread) to [thread blocks](/device-software/thread-block) to [thread block grids](/device-software/thread-block-grid) (left), mapping onto the hardware from [CUDA Cores](/device-hardware/cuda-core) to [Streaming Multiprocessors](/device-hardware/streaming-multiprocessor) to the entire GPU (right). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](https://modal-cdn.com/gpu-glossary/terminal-cuda-programming-model.svg)

The thread hierarchy is a key abstraction of the [CUDA programming model](/device-software/cuda-programming-model), alongside the [memory hierarchy](/device-software/memory-hierarchy). It organizes the execution of parallel programs across multiple levels, from individual threads up to entire GPU devices.

At the lowest level are individual [threads](/device-software/thread). Like a thread of execution on a CPU, each CUDA thread executes a stream of instructions. The hardware resources that effect arithmetic and logic instructions are called [Cores](/device-hardware/core). Threads are selected for execution by the [Warp Scheduler](/device-hardware/warp-scheduler).

The intermediate level consists of [thread blocks](/device-software/thread-block), which are also known as [cooperative thread arrays](/device-software/cooperative-thread-array) in [PTX](/device-software/parallel-thread-execution) and [SASS](/device-software/streaming-assembler). Each [thread](/device-software/thread) has a unique identifier within its [thread block](/device-software/thread-block). These thread identifiers are index-based, to support assignment of work to threads based on indices into input or output arrays. All threads within a block are scheduled simultaneously onto the same [Streaming Multiprocessor (SM)](/device-hardware/streaming-multiprocessor). They can coordinate through [shared memory](/device-software/shared-memory) and synchronize with barriers.

At the highest level, multiple [thread blocks](/device-software/thread-block) are organized into a [thread block grid](/device-software/thread-block-grid) that spans the entire GPU. [Thread blocks](/device-software/thread-block) are strictly limited in their coordiation and communication. Blocks within a grid execute concurrently with respect to each other, with no guaranteed execution order. CUDA programs must be written so that any interleaving of blocks is valid, from fully serial to fully parallel. That means thread blocks](/device-software/thread-block) cannot, for instance, synchronize with barriers. Like [threads](/device-software/thread), each [thread block](/device-software/thread-block) has a unique, index-based identifier to support assignment of work based on array index.

This hierarchy maps directly onto the [GPU hardware](/device-hardware): [threads](/device-software/thread) execute on individual [cores](/device-hardware/core), [thread blocks](/device-software/thread-block) are scheduled onto [SMs](/device-hardware/streaming-multiprocessor), and [grids](/device-software/thread-block-grid) utilize all available [SMs](/device-hardware/streaming-multiprocessor) on the device.
